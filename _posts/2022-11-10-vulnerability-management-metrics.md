---
layout: post
description: Vulnerability Management Metrics
comments: true
date: 2022-11-10
last-update: 2022-10-26
---

WiP

I took these metrics because I have been using them to drive vulnerability management at scale in my former job.
In this document I tried to generalize, abstract, and formalize those metrics as much as possible, so that I could discuss whys more hows.

---

# Table of Contents

- [Table of Contents](#table-of-contents)
- [What is Vulnerability Management](#what-is-vulnerability-management)
- [Executive metrics](#executive-metrics)
  - [Risk](#risk)
    - [Simple Risk](#simple-risk)
    - [Triaged Risk](#triaged-risk)
    - [Enhanced Risk](#enhanced-risk)
    - [Normalizing the Risk](#normalizing-the-risk)
    - [Defining what good looks like for Risk](#defining-what-good-looks-like-for-risk)

# What is Vulnerability Management

You can find a general definition of Vulnerability Management in many places, like [Wikipedia](https://en.wikipedia.org/wiki/Vulnerability_management).
But, for the sake of this document, I am considering the definition adopted by the vast majority of medium and large companies.

For the rest of this note, I am assuming that:

> Vulnerability Management is the set of processes and mechanisms used to _manage the risk_ posed to the company by _known software vulnerabilities_.

In this simple statement, there are two important concepts that determine the scope and goals of this very complicated world: risk management and known vulnerabilities.

We shall see in details what these mean, providing some ideas on how to measure the effectiveness of a Vulnerability Management program.

# Executive metrics

Running a Vulnerability Management program is expensive. So, if you want to make sure what you are doing is efficient, you need effective metrics.

Even though a plethora of metrics can be used, you want to tell at a glance where you are and your progress. Moreover, you definitely don't want to drown in an ocean of graphs and numbers or present your VP the details about how you run things.

We can identify two _first order_, or _executive_, metrics: **_Risk and Coverage_**.

In this document, we will be discussing these two metrics first. Then we will explore _second order_, or _operative_, metrics that we can define also as _leading_ metrics for either Risk or Converge. These can be used to identify bottlenecks and prioritize the work needed to bring Risk down or improve Coverage.

## Risk

When managing vulnerabilities at scale, it is crucial to evaluate the realistic risk posed by known vulnerabilities.

With Risk, we want to have an idea of how easy it would be for an attacker to leverage vulnerabilities on our software assets to damage the company.
In the diverse and ample spectrum of the possible damages a company can get from security incidents, are stealing of trade secrets, user data compromise, or image damage which can just be caused by exposing a bad management of security...

So, we want to make sure we consider all known vulnerabilities, along with the likelihood and the impact of them being exploited.

It is impractical for the typical attacker to enumerate all the possible vulnerabilities to find the "perfect one". A widespread vulnerability will be easier to spot.

Another important reason why you should adopt a Risk metric is prioritization of remediation. When dealing with security risk, it's always a matter of where and how you take trade-offs: you can almost never fix everything, and the influx of vulnerabilities never stops. You never have infinite resources to invest in vulnerabilities remediation, also because that it is not the core business of your company.

So, having an _effective way to prioritize security remediation_ is paramount.

For instance, remediating a severe vulnerability affecting a single asset should not be more urgent than remediating many assets vulnerable to different vulnerability, with a similar severity score.

In the rest of this chapter, we will see three risk metrics, starting with a simple definition and ending with a refined one. As you shall see, perfection is unattainable, still it is worth pursuing the best possible metric we can afford.

### Simple Risk

In its simplest (and often times useless) form, Risk can be quantified as the sum of the product of number of affected assets by the Common Vulnerability Scoring System score ([CVSS, as defined by NIST](https://www.first.org/cvss/user-guide)).

The number of assets affected by a vulnerability represents the _potential attack surface_ for it. The score represents the risk posed by a vulnerability on a single software instance.

More formally, we can define the simple Risk ($sRisk$) for a given vulnerability $v$ in the set of known vulnerabilities $V$ as:

$$ sRisk(v) = |assets(v)| \cdot  cvss(v), v \in V $$

Or, if we want to calculate the aggregated risk on all the software we own:

$$ sRisk = \sum\limits_{v \in V} sRisk(v) $$

In both cases, $assets(v)$ is the set of assets (i.e., software instances) affected by the vulnerability $v$, and $cvss(v)$ is the CVSS Score of $v$.

In practical terms, the set of known vulnerabilities is given by the vulnerability scanners used.

### Triaged Risk

In large companies, you don't want to send urgent tickets to your engineers to resolve non-existent vulnerabilities.

Many times, the risk posed by a software vulnerability can be better evaluated considering how that particular software instance is compiled, configured and used.
In other words, _triaging of vulnerabilities_ could improve our understanding of the security risk and help to modulate remediation efforts and investments.

The definition of Risk we gave before does not account for automatic or manual triage. We almost always want to use a risk score enriched with information on the actual usage of software and potential mitigations that are in place.

**Automatic triage**
Automatic triage is what many vulnerability scanners do: they often try to give a better risk score, and reduce the number of false positives, by taking into account the context and how the software is used and the mitigations in place (e.g., AppArmor or SELinux).

For instance, RedHat provides an [adjusted score for RHEL](https://access.redhat.com/security/updates/classification) (RedHat Enterprise Linux) packages. As you can read in their documentation:

> [...] NVD may rate a flaw in a particular service as having High Impact on the CVSS CIA Triad (Confidentiality, Integrity, Availability) where the service in question is typically run as the root user with full privileges on a system. However, in a Red Hat product, the service may be specifically configured to run as a dedicated non-privileged user running entirely in a SELinux sandbox, greatly reducing the immediate impact from compromise, resulting in Low impact.

Many vendors sell vulnerability feeds with additional and refined information about vulnerabilities. [Snyk](https://snyk.io), [Accenture iDefense](https://www.accenture.com/_acnmedia/pdf-71/accenture-idefense-intelgraph.pdf), [Flexera](https://www.flexera.com), and [FireEye](https://www.fireeye.com) are some example.
You can implement your own automated triage pipeline, using readily available, and open source, tools like [nvdtool](https://github.com/facebookincubator/nvdtools).

**Manual triage**
In environments where there is a high level of "standardization" and customization (i.e. most hyper-scale companies), human triage can be also used. 
Security analysts reviews vulnerabilities and manually adjust (up or down) the CVSS score. This is done typically by tweaking the [CVSS vector string](https://www.first.org/cvss/calculator/3.0) according to the context.

To explain this better, let's consider a real-life scenario.
[CVE-2020-12284](https://nvd.nist.gov/vuln/detail/CVE-2020-12284) affects [FFmpg](https://www.ffmpeg.org) 4.1 and 4.2.2, but we know our version is not compiled with anything using `cbs_jpeg_split_fragment` in `libavcodec/cbs_jpeg.c`.
The problem is still there, so it would be nice to have an upgrade, but upgrading FFmpeg is less urgent than mitigating an exploitable vulnerability, so we can "temporarily downgrade" the score.

Vice versa, there could be situations in which we might want to bump up the risk score, because a given software is configured in a way that makes the impacts of an exploit even more dangerous...

**Triaged Risk calculation**
We can define the triaged Risk, $tRisk$, for a vulnerability $v \in V$, as

$$ tRisk(v) = |assets(v)| \cdot tscore(v)$$

where $tscore(v)$ is the triage score for $v$. This is a function of the CVSS score and any other automatic and manual assessment regarding $v$.

It is also usful to calculate the aggregated triaged Risk on all the assets (e.g., machines). This can be calculated as follows:

$$ tRisk = \sum\limits_{v \in V} tRisk(v) $$

In a realistic scenario, $V$ is the set of vulnerabilities discovered by your scanners, and $tscore(v)$ is the score assigned by the scanner to $v$.

### Enhanced Risk

In more mature vulnerability programs, Risk should consider even more details.

For instance, we may want to take into account:

- Temporal factors - how long the vulnerability has been around?
  - The exploitation of a very old vulnerability, even with minimal quantifiable damage, can give your users the perception that you don't care about security...
- Intelligence data
  - is this vulnerability being actively exploited by [APT](https://en.wikipedia.org/wiki/Advanced_persistent_threat) (Advanced Persistent Threat) against similar companies (see [Operation Aurora](https://en.wikipedia.org/wiki/Operation_Aurora))
  - is our intelligence/red-team aware of exploits?
- Blast radius - is this vulnerability affecting highly-sensitive environments?

We can enrich the previous risk score definition with any additional information, in an automated or manual way. We call this new Risk definition _Enhanced Risk_ ($eRisk$), and calculate it as:

$$ eRisk(v) = escore(v) \cdot \sum\limits_{a \in assets(v)} br(a) \cdot ex(a) $$

So, we can now calculate the aggregated enhanced risk as:

$$ eRisk = \sum\limits_{v \in V} eRisk(v) $$

Where $br(a)$ is the blast radius of $a$, and $ex(a)$ is the exposure of the asset $a$ to threats.

The range of values $br()$ and $ex()$ can take should be devised so that we can effectively "bump up" vulnerabilities remediation priority as needed.
In other words if $eRisk(v') > eRisk(v'')$, remediation of $v'$ should take priority over remediation of $v''$.

Just as an example, we could say that:

$$ \forall a \in assets(v) \quad  br(a) \in [1,1.5] \quad \textrm{and} \quad ex(a) \in [1,1.5] $$

With $br(a) = 1 \ \text{and} \ ex(a) = 1$ if we don't have any reason to bump up or down the "importance" of vulnerabilities affecting the asset $a$.

Finally, $escore(v)$ is the enhanced score for $v$. It should consider aspects that can affect the [CVSS vector string](https://www.first.org/cvss/calculator/3.0), such as:

- manual triage;
- adjusted CVSS score provided by vendors (e.g., RH adjusted score);
- intelligence information such as known exploitation in the wild;
- age of the vulnerability (time elapsed from the time of public or intelligence disclosure).

### Normalizing the Risk

In large companies the number of assets grows and shrink frequently in time, and so does the Risk, according to the definition we gave before.

While that definition can be used to prioritize triaging and remediation efforts, within and across different domains, it makes it hard to track the progress of a vulnerability management program over time.

Our Risk executive metric needs to be normalized on the number of assets considered in the reference period. 
So, we can define the normalized, triaged, risk at time $t$ as:

$$ ntRisk^t = \frac{tRisk^t}{|assets^t|} $$

Where $assets^t$ is the set of all assets on which we are running the vulnerability management program at time $t$, and $tRisk^t$ is the triaged risk at time $t$.

In the same way, we can calculate the normalized, enhanced, risk at time $t$ as:

$$ neRisk^t = \frac{eRisk^t}{|assets^t|} $$

The granularity of time, depends on the frequency vulnerability scanners are run. For very large domains, we could only be able to track progress every couple of days or every week.

### Defining what good looks like for Risk

Given the definition of normalized risk above, we can track progress by comparing, for instance, $ntRisk^{t1}$ and $ntRisk^{t2}$.
If $t2 > t1$ and $ntRisk^{t1} > ntRisk^{t2}$, we can say that we are making progress in the right direction.

We can set a target for $ntRisk$, considering anything below it acceptable residual risk.
Or, we can define risk bands: `low, medium, high, critical`. And provide a very high level idea of the risk our company is subject to.

As you probably have realized, the quality and freshness of the asset inventory is critical to have a meaningful Risk score.
We will see in the chapter about Coverage that inventories are a typical problem for large companies. You should invest lots of your resources in doing inventory right, before taking any risk score too seriously.
